{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6182c51c",
      "metadata": {},
      "source": [
        "<img src=\"../media/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlflow-explanation",
      "metadata": {},
      "source": [
        "# MLflow\n",
        "\n",
        "## Simple Model Deployment and Evaluation\n",
        "\n",
        "Let's walk through a basic workflow demonstrating how MLflow can be used to track experiments, log parameters, metrics, and models for a simple classification task.\n",
        "\n",
        "### What is MLflow?\n",
        "\n",
        "MLflow is an open-source platform for managing the end to end machine learning lifecycle. It addresses four primary functions:\n",
        "\n",
        "* **Tracking:** Record and query experiments (code, data, config, results).\n",
        "\n",
        "* **Projects:** Package ML code in a reusable, reproducible form.\n",
        "\n",
        "* **Models:** Manage and deploy models from various ML libraries to diverse serving platforms.\n",
        "\n",
        "* **Model Registry:** Centralized model store to collaboratively manage the full lifecycle of MLflow Models.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "This example demonstrates a basic MLflow workflow for a binary classification problem using a Decision Tree Classifier. It covers:\n",
        "\n",
        "1. **Setting up an MLflow Experiment:** Organizing runs under a common name.\n",
        "\n",
        "2. **Starting an MLflow Run:** Creating a specific instance of an experiment.\n",
        "\n",
        "3. **Data Preparation:** Generating synthetic data for a simplified binary classification.\n",
        "\n",
        "4. **Logging Parameters:** Recording key data split parameters and model hyperparameters.\n",
        "\n",
        "5. **Model Training:** Training a `DecisionTreeClassifier` on the prepared data.\n",
        "\n",
        "6. **Calculating and Logging Metrics:** Evaluating the model's performance (accuracy, precision, recall, F1-score) and logging these metrics.\n",
        "\n",
        "7. **Logging the Model:** Saving the trained model as an MLflow artifact, making it ready for deployment.\n",
        "\n",
        "This setup allows for easy comparison of different runs, understanding the impact of various parameters, and reproducing results.\n",
        "\n",
        "### Installation & Setup\n",
        "\n",
        "First, you'd install MLflow in your Python environment:\n",
        "\n",
        "* `pip install mlflow`\n",
        "\n",
        "Lets review a simple classification example where we create some data and applies a decision tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019c8d67",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:25:37 INFO mlflow.tracking.fluent: Experiment with name 'Decision_Tree_Classification_Analysis_Simplified' does not exist. Creating a new experiment.\n",
            "2025/08/18 11:25:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:25:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow Run completed for Experiment '686135398512945305' and Run '925db5616b1948c294d9cfcce59b04b3'\n",
            "Logged Data Split: Test Ratio=0.2, Stratified=True\n",
            "Logged Model Parameters: Max Depth=4, Min Samples Leaf=3, Criterion=entropy\n",
            "Logged Metrics: Accuracy=0.8000, Precision=0.7500, Recall=0.7500, F1-Score=0.7500\n",
            "Model saved as artifact: decision_tree_classifier_model\n",
            "To view this run, run 'mlflow ui' in your terminal in this directory.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a36a1905",
      "metadata": {},
      "source": [
        "**How to see your results: The MLflow UI**\n",
        "- After running this script, open your terminal in the directory where your 'mlruns' folder is located.\n",
        "- Then, execute:\n",
        "    - `mlflow ui`\n",
        "    - This will launch the MLflow Tracking UI, usually accessible at http://localhost:5000.\n",
        "    - You'll be able to navigate to the \"Decision_Tree_Classification_Analysis\" experiment and inspect the details of \"Decision_Tree_Trial_1\", including its parameters, metrics, and the saved model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf74cd0a",
      "metadata": {},
      "source": [
        "### MLflow Tracking Cheat Sheet: Essential Logging Functions\n",
        "\n",
        "Here a table that summarises the most common and essential `mlflow.log_*` functions, along with their purpose and examples. This will serve as a quick reference for effectively leveraging MLflow Tracking.\n",
        "\n",
        "| MLflow Function | Purpose | When to Use | Example |\n",
        "| :--------------------------- | :------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------- |\n",
        "| **`mlflow.log_param(key, value)`** | **Logs a single key-value parameter.** Values are typically strings, numbers, or booleans. Used for hyperparameters, configuration settings, or data characteristics. | To record **input settings** or **configurations** that define a specific run. Essential for reproducibility and comparison. | `mlflow.log_param(\"learning_rate\", 0.01)` <br> `mlflow.log_param(\"optimizer\", \"Adam\")` <br> `mlflow.log_param(\"data_scaling_method\", \"MinMaxScaler\")` |\n",
        "| **`mlflow.log_metric(key, value)`** | **Logs a single key-value metric.** Values must be numeric. MLflow tracks metrics as they change over time (e.g., during training epochs). | To record **quantitative performance indicators** of your model or experiment. Essential for comparing run performance. | `mlflow.log_metric(\"accuracy\", 0.925)` <br> `mlflow.log_metric(\"validation_loss\", 0.15, step=10)` (for epoch-based logging) <br> `mlflow.log_metric(\"f1_score\", 0.88)` |\n",
        "| **`mlflow.log_artifact(local_path, artifact_path=None)`** | **Logs a single file as an artifact.** The file is copied from `local_path` to the run's artifact URI. | To save **important output files** that are not parameters or metrics but provide context or are useful for later analysis/reproducibility. | `mlflow.log_artifact(\"plots/roc_curve.png\")` <br> `mlflow.log_artifact(\"my_dataset_sample.csv\", artifact_path=\"data\")` |\n",
        "| **`mlflow.log_artifacts(local_dir, artifact_path=None)`** | **Logs an entire directory of files as artifacts.** All files within `local_dir` are recursively copied. | To save **multiple related output files** at once (e.g., all plots from a run, all config files). | `mlflow.log_artifacts(\"results_folder\")` <br> `mlflow.log_artifacts(\"configs/\", artifact_path=\"model_configs\")` |\n",
        "| **`mlflow.<flavor>.log_model(model, artifact_path, ...)`** | **Logs a machine learning model specific to a framework/library (flavor).** This function packages the model along with necessary metadata, code dependencies, and often a signature. | To save your **trained ML model** in a standardized format that MLflow can later reload, serve, and deploy using various tools. **This is crucial for model management!** | `mlflow.sklearn.log_model(my_sklearn_model, \"my_classification_model\")` <br> `mlflow.pytorch.log_model(my_torch_model, \"pytorch_model\")` <br> `mlflow.tensorflow.log_model(my_tf_model, \"tf_model\")` |\n",
        "| **`mlflow.set_experiment(experiment_name)`** | **Sets the current active experiment.** If the experiment doesn't exist, it's created. All subsequent runs will belong to this experiment. | To **organize your runs** into logical groups. This is typically one of the first lines of MLflow code in a script. | `mlflow.set_experiment(\"Customer_Churn_Prediction\")` <br> `mlflow.set_experiment(\"Image_Recognition_CNN_Trials\")` |\n",
        "| **`mlflow.set_tag(key, value)`** | **Adds a tag (key-value pair) to the current run.** Tags are flexible strings used for arbitrary metadata. | To add **descriptive or categorizing metadata** that doesn't fit into parameters or metrics (e.g., developer name, data version, specific algorithm variant). | `mlflow.set_tag(\"developer\", \"Alice\")` <br> `mlflow.set_tag(\"data_version\", \"v2.1_cleaned\")` <br> `mlflow.set_tag(\"model_family\", \"tree_based\")` |\n",
        "| **`mlflow.start_run(run_name=None, ...)`** | **Starts an MLflow run and returns an `ActiveRun` object.** Typically used with a `with` statement for automatic run termination. | To **begin tracking a new experiment run.** This is the core context manager for capturing run information. | `with mlflow.start_run(run_name=\"Hyperparam_Search_Trial_1\"):` <br> `  # Your ML code here` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiple-models-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Multiple Models Comparison\n",
        "\n",
        "## Comparing Different Classification Models on the Iris Dataset\n",
        "\n",
        "This section demonstrates how to use MLflow to track and compare the performance of multiple machine learning models within a single experiment. We will use the classic **Iris dataset** for a multi-class classification problem.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary goal of this example is to illustrate MLflow's capability to facilitate model comparison by logging distinct runs for different models under a unified experiment. Specifically, it covers:\n",
        "\n",
        "1.  **Experiment Setup:** Defining a new MLflow experiment (`Iris Classification Models`) to group all runs related to Iris classification.\n",
        "2.  **Dataset Loading and Splitting:** Loading the Iris dataset and splitting it into training and testing sets.\n",
        "3.  **Iterative Model Training and Logging:** Training three different classification models:\n",
        "    * **K-Nearest Neighbors (KNN)**\n",
        "    * **Support Vector Machine (SVM)**\n",
        "    * **Decision Tree Classifier**\n",
        "4.  **Parameter Tracking:** For each model, relevant hyperparameters (e.g., `n_neighbors` for KNN, `C` and `kernel` for SVM, `max_depth` for Decision Tree) are logged using `mlflow.log_param()`.\n",
        "5.  **Metric Logging:** Standard classification metrics (Accuracy, Precision, Recall, F1-Score) are calculated on the test set and logged for each model using `mlflow.log_metric()`. The `average='weighted'` parameter is used for precision, recall, and F1 score to handle the multi class nature of the Iris dataset.\n",
        "6.  **Model Artifacts:** Each trained model is saved as an MLflow artifact using `mlflow.sklearn.log_model()`, allowing for easy retrieval and deployment later.\n",
        "\n",
        "By logging each model's parameters and performance metrics within separate runs under the same experiment, MLflow provides a clear interface to compare their effectiveness and identify the best performing model for the given task.\n",
        "\n",
        "### How it Works\n",
        "\n",
        "The code uses a `with mlflow.start_run(...)` block for each model. This ensures that all parameters, metrics, and the model logged within that block are associated with that specific run. The experiment name is set once at the beginning, grouping all these individual model runs together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6430670",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:27:25 INFO mlflow.tracking.fluent: Experiment with name 'Iris Classification Models' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Iris Classification Model Comparison...\n",
            "\n",
            "--- Training KNN Classifier ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:27:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:27:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  KNN Metrics: Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "  KNN Model saved to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/339252108759577855/4b9564f77bee4c86a3aa76e2473e11d0/artifacts/knn_model\n",
            "\n",
            "--- Training SVM Classifier ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:27:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:27:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SVM Metrics: Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "  SVM Model saved to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/339252108759577855/7bad9fa1ee9e4426975eb8f590a062d8/artifacts/svm_model\n",
            "\n",
            "--- Training Decision Tree Classifier ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:27:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:27:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Decision Tree Metrics: Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "  Decision Tree Model saved to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/339252108759577855/e620c67de48844509a4f80643182b2d8/artifacts/decision_tree_model\n",
            "\n",
            "All Iris classification models trained and logged to MLflow.\n",
            "To view results, run 'mlflow ui' in your terminal and navigate to the 'Iris Classification Models' experiment.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d3d4b0d5",
      "metadata": {},
      "source": [
        "In essence, this code runs three separate machine learning classification experiments on the same dataset (Iris) and uses MLflow to automatically record all the key details—the model's settings (parameters), its performance (metrics), and the trained model itself (artifacts)—for each experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "titanic-mlflow-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Preprocessing, Multiple Models, and Cross-Validation\n",
        "\n",
        "## Robust Machine Learning Workflow for Titanic Survival Prediction\n",
        "\n",
        "This example demonstrates a comprehensive machine learning workflow for the Kaggle Titanic survival prediction problem. It focuses on building robust, evaluable, and traceable models by incorporating data preprocessing, comparing multiple model types, and utilizing cross-validation, all tracked meticulously with MLflow.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The main objective is to showcase an end-to-end ML pipeline within the MLflow framework, covering:\n",
        "\n",
        "* **Data Loading and Initial Cleaning:** Handling the input CSV files and dropping irrelevant columns like `PassengerId`, `Name`, `Ticket`, and `Cabin`.\n",
        "\n",
        "* **Automated Preprocessing Pipelines:** Defining and applying robust preprocessing steps for both numerical and categorical features. This includes:\n",
        "    * **Numerical Features (`Age`, `Fare`, `SibSp`, `Parch`):** Imputing missing values with the mean and scaling features using `StandardScaler`.\n",
        "    * **Categorical Features (`Pclass`, `Sex`, `Embarked`):** Imputing missing values with the most frequent value and converting categorical data into numerical format using `OneHotEncoder`.\n",
        "    * These steps are encapsulated within `sklearn.pipeline.Pipeline` and combined using `sklearn.compose.ColumnTransformer` for a streamlined process.\n",
        "\n",
        "* **MLflow Experiment Setup:** Setting up a dedicated MLflow experiment (`Titanic CV & Model Logging`) to group all runs related to this project, ensuring organized tracking.\n",
        "\n",
        "* **Cross-Validation for Robust Evaluation:** Employing `StratifiedKFold` cross-validation to assess model performance more reliably across different subsets of the training data. This helps in getting a more generalized estimate of the model's performance and reducing overfitting bias.\n",
        "\n",
        "* **Multiple Model Comparison:** Training and evaluating three distinct classification algorithms:\n",
        "    * **Logistic Regression:** A linear model for binary classification.\n",
        "    * **Random Forest Classifier:** An ensemble tree-based method known for its robustness.\n",
        "    * **Gradient Boosting Classifier:** Another powerful ensemble method that builds trees sequentially.\n",
        "\n",
        "* **Comprehensive MLflow Logging:** For each model and cross-validation run, MLflow logs are captured:\n",
        "    * **Parameters:** Model hyperparameters, data split configurations, and preprocessing details.\n",
        "    * **Metrics:** Mean and standard deviation of cross-validation scores (accuracy, precision, recall, F1-score) to provide a statistical summary of performance.\n",
        "    * **Models as Artifacts:** The entire `sklearn.pipeline.Pipeline` (including preprocessing and the final classifier) is logged as an MLflow artifact, making it easy to reproduce predictions or deploy the exact trained workflow.\n",
        "\n",
        "This structured approach ensures that every aspect of the model development process is recorded, enabling easy comparison, reproducibility, and auditing of machine learning experiments.\n",
        "\n",
        "### Data Requirements\n",
        "\n",
        "This script expects `train.csv` and `test.csv` files from the Kaggle Titanic competition to be present in a `data/` subdirectory relative to where the script is run. If these files are not found, the script will exit and provide instructions for downloading them.\n",
        "\n",
        "### How to See Your Results: The MLflow UI\n",
        "\n",
        "After running this script, you can visualize and compare all the logged experiments and runs using the MLflow Tracking UI:\n",
        "\n",
        "-   Open your terminal in the directory where your 'mlruns' folder is located (this is usually the directory from which you ran the Python script).\n",
        "-   Execute the command:\n",
        "    -   `mlflow ui`\n",
        "-   This will launch the MLflow Tracking UI, typically accessible in your web browser at `http://localhost:5000`.\n",
        "-   You'll be able to navigate to the `Titanic CV & Model Logging` experiment and inspect the details of each model's run, including logged parameters, metrics (mean and std of CV scores), and the saved `final_model_pipeline` artifact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f586ebf8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:28:07 INFO mlflow.tracking.fluent: Experiment with name 'Titanic CV & Model Logging' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Titanic Classification with CV and Model Logging...\n",
            "\n",
            "--- Training Logistic Regression with Cross-Validation ---\n",
            "  Logistic Regression CV Results:\n",
            "    Mean Test accuracy: 0.8014 (Std: 0.0134)\n",
            "    Mean Test precision_weighted: 0.8002 (Std: 0.0136)\n",
            "    Mean Test recall_weighted: 0.8014 (Std: 0.0134)\n",
            "    Mean Test f1_weighted: 0.7988 (Std: 0.0135)\n",
            "  Fitting final Logistic Regression pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:28:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:28:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Logistic Regression pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/2cc6a08b7da34dbfb96410689b249dc2/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Random Forest Classifier with Cross-Validation ---\n",
            "  Random Forest Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8406 (Std: 0.0170)\n",
            "    Mean Test precision_weighted: 0.8418 (Std: 0.0179)\n",
            "    Mean Test recall_weighted: 0.8406 (Std: 0.0170)\n",
            "    Mean Test f1_weighted: 0.8374 (Std: 0.0177)\n",
            "  Fitting final Random Forest Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:28:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:28:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Random Forest Classifier pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/ba6a34762a5246bfbfb01e11398a6214/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Gradient Boosting Classifier with Cross-Validation ---\n",
            "  Gradient Boosting Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8227 (Std: 0.0152)\n",
            "    Mean Test precision_weighted: 0.8235 (Std: 0.0145)\n",
            "    Mean Test recall_weighted: 0.8227 (Std: 0.0152)\n",
            "    Mean Test f1_weighted: 0.8191 (Std: 0.0167)\n",
            "  Fitting final Gradient Boosting Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:28:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:28:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Gradient Boosting Classifier pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/0e5c8ffd889f477a92395d3999379741/artifacts/final_model_pipeline\n",
            "\n",
            "All Titanic models processed with CV and models logged to MLflow.\n",
            "To view results, run 'mlflow ui' in your terminal and navigate to the 'Titanic CV & Model Logging' experiment.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "17b05c87",
      "metadata": {},
      "source": [
        "In essence, this code sets up a robust machine learning workflow for the Titanic dataset that automatically handles data preparation and evaluates multiple models using cross-validation, while meticulously logging all parameters, cross-validation metrics, and the final trained models to MLflow for easy comparison and reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d35723",
      "metadata": {},
      "source": [
        "After some refractoring: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28472335",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Titanic Classification with CV and Model Logging...\n",
            "\n",
            "--- Training Logistic Regression with Cross-Validation ---\n",
            "  Logistic Regression CV Results:\n",
            "    Mean Test accuracy: 0.8014 (Std: 0.0134)\n",
            "    Mean Test precision_weighted: 0.8002 (Std: 0.0136)\n",
            "    Mean Test recall_weighted: 0.8014 (Std: 0.0134)\n",
            "    Mean Test f1_weighted: 0.7988 (Std: 0.0135)\n",
            "  Fitting final Logistic Regression pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:29:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:29:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Logistic Regression pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/7ddf2f9e6a6342b7a5b1edae00cbef76/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Random Forest Classifier with Cross-Validation ---\n",
            "  Random Forest Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8406 (Std: 0.0170)\n",
            "    Mean Test precision_weighted: 0.8418 (Std: 0.0179)\n",
            "    Mean Test recall_weighted: 0.8406 (Std: 0.0170)\n",
            "    Mean Test f1_weighted: 0.8374 (Std: 0.0177)\n",
            "  Fitting final Random Forest Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:29:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:29:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Random Forest Classifier pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/83c78febbc834cb2a6ca4c5812c4859e/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Gradient Boosting Classifier with Cross-Validation ---\n",
            "  Gradient Boosting Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8227 (Std: 0.0152)\n",
            "    Mean Test precision_weighted: 0.8235 (Std: 0.0145)\n",
            "    Mean Test recall_weighted: 0.8227 (Std: 0.0152)\n",
            "    Mean Test f1_weighted: 0.8191 (Std: 0.0167)\n",
            "  Fitting final Gradient Boosting Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:29:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:29:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Gradient Boosting Classifier pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/be1913d16ac84f84861592dfddbc78ff/artifacts/final_model_pipeline\n",
            "\n",
            "All Titanic models processed with CV and models logged to MLflow.\n",
            "To view results, run 'mlflow ui' in your terminal and navigate to the 'Titanic CV & Model Logging' experiment.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5900b18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Titanic Classification with CV and Model Logging...\n",
            "\n",
            "--- Training Logistic Regression with Cross-Validation ---\n",
            "  Logistic Regression CV Results:\n",
            "    Mean Test accuracy: 0.8014 (Std: 0.0134)\n",
            "    Mean Test precision_weighted: 0.8002 (Std: 0.0136)\n",
            "    Mean Test recall_weighted: 0.8014 (Std: 0.0134)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:38:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Mean Test f1_weighted: 0.7988 (Std: 0.0135)\n",
            "  Fitting final Logistic Regression pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:38:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Logistic Regression pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/67c18e4379c54d7899b655e38e40cb33/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Random Forest Classifier with Cross-Validation ---\n",
            "  Random Forest Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8406 (Std: 0.0170)\n",
            "    Mean Test precision_weighted: 0.8418 (Std: 0.0179)\n",
            "    Mean Test recall_weighted: 0.8406 (Std: 0.0170)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:38:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Mean Test f1_weighted: 0.8374 (Std: 0.0177)\n",
            "  Fitting final Random Forest Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:38:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Random Forest Classifier pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/ef610db8f47e445fa9c98da1a72d0f6e/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Gradient Boosting Classifier with Cross-Validation ---\n",
            "  Gradient Boosting Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8227 (Std: 0.0152)\n",
            "    Mean Test precision_weighted: 0.8235 (Std: 0.0145)\n",
            "    Mean Test recall_weighted: 0.8227 (Std: 0.0152)\n",
            "    Mean Test f1_weighted: 0.8191 (Std: 0.0167)\n",
            "  Fitting final Gradient Boosting Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/18 11:38:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/18 11:38:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Gradient Boosting Classifier pipeline logged to: file:///c:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA_MLOPs_02_COACH_POETRY/04_ML_Flow/mlruns/614299104235584322/2ed59a466b8b4e2da69204c8bd64086a/artifacts/final_model_pipeline\n",
            "\n",
            "All Titanic models processed with CV and models logged to MLflow.\n",
            "To view results, run 'mlflow ui' in your terminal and navigate to the 'Titanic CV & Model Logging' experiment.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88c9ac9d344d499bb2709c253042414d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not demonstrate model loading: Failed to download artifacts from path 'model', please ensure that the path is correct.\n",
            "Ensure you have run the script successfully and an MLflow run exists.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "calling-mlflow-single-prediction-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Calling a Logged Model - Single Data Point\n",
        "\n",
        "## Demonstrating Model Loading and Prediction for a Single Entry\n",
        "\n",
        "This section illustrates how to load a previously logged MLflow model and use it to make a prediction for a single new, unseen data point. This is a fundamental step in the model deployment and inference phase of the machine learning lifecycle, often used for real time predictions or individual queries.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary objective is to demonstrate the ease with which MLflow allows you to retrieve and utilize a trained model artifact. Specifically, it covers:\n",
        "\n",
        "1.  **Identifying the Model:** How to specify the exact model to load using its MLflow Run ID and the artifact path under which it was saved.\n",
        "2.  **Constructing the Model URI:** Forming the unique MLflow URI (`runs:/<RUN_ID>/<ARTIFACT_PATH>`) that points directly to the logged model.\n",
        "3.  **Loading the Model:** Using `mlflow.sklearn.load_model()` (or the appropriate flavor-specific `load_model` function) to deserialize the model pipeline into memory.\n",
        "4.  **Making a Single Prediction:** Applying the loaded model to a single new input data row to generate a prediction.\n",
        "5.  **Error Handling:** Including a `try-except` block to gracefully handle scenarios where the model cannot be loaded, providing helpful debugging information.\n",
        "\n",
        "This process is fundamental for integrating MLflow managed models into production systems, especially for scenarios requiring individual predictions.\n",
        "\n",
        "### Key Considerations\n",
        "\n",
        "* **`MLFLOW_RUN_ID`:** This is the unique identifier for the specific MLflow run during which your desired model was trained and logged. You can find this ID in the MLflow UI or from the output of the training script (e.g., from the `mlflow.active_run().info.run_id` printout).\n",
        "* **`MODEL_ARTIFACT_PATH`:** This is the name you gave to the model when it was logged (e.g., `\"final_model_pipeline\"` or `\"decision_tree_classifier_model\"`). It's the sub-path within the run's artifact directory.\n",
        "* **Input Data Format:** The new data (`new_passenger` in this example) used for prediction **must** have the same feature columns and data types as the data used to train the original model. If the original model included a preprocessing pipeline (as in the Titanic example), the loaded object will be the entire pipeline, and you should pass raw, un-preprocessed data to it as a Pandas DataFrame, even for a single row.\n",
        "* **Environment:** Ensure that the Python environment where you are loading the model has all the necessary libraries installed (e.g., `scikit-learn`, `pandas`, `mlflow`) that were used when the model was originally saved.\n",
        "\n",
        "This example uses a hardcoded `MLFLOW_RUN_ID` for demonstration purposes. In a real world scenario, this ID might be dynamically retrieved from a model registry, a database, or a configuration file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184eebef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ca0bf20f1a7407fab63c9c77222d2a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8799c927983d43b08963f24475e3fef3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating loaded model prediction from Run ID: e29e2b05b8e341d7809c89725c6797e9 ---\n",
            "Loaded pipeline from MLflow URI: runs:/e29e2b05b8e341d7809c89725c6797e9/model\n",
            "New passenger data:\n",
            "   Pclass   Sex   Age  SibSp  Parch  Fare Embarked\n",
            "0       3  male  28.0      0      0  10.0        S\n",
            "Predicted Survival (0=No, 1=Yes): 0\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "calling-mlflow-multiple-predictions-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Calling a Logged Model - Multiple Data Points\n",
        "\n",
        "## Demonstrating Model Loading and Prediction for Multiple Entries\n",
        "\n",
        "This section extends the previous example to show how to load an MLflow model and use it to make predictions for an array or a batch of new, unseen data points. This is a common scenario for batch processing or when serving multiple requests simultaneously.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary objective is to demonstrate how to efficiently use a loaded MLflow model to predict outcomes for multiple inputs. Specifically, it covers:\n",
        "\n",
        "1.  **Re-loading the Model:** The process of loading the model remains the same, emphasizing that the loaded object can handle multiple rows of input.\n",
        "2.  **Preparing Batch Input:** Creating a Pandas DataFrame containing several new data points, ensuring that the structure (column names and types) matches the expectations of the trained model/pipeline.\n",
        "3.  **Batch Prediction:** Applying the `predict()` method of the loaded model to the entire DataFrame of new data, which returns an array of predictions.\n",
        "4.  **Integrating Predictions:** Demonstrating how to add the predictions back into the input DataFrame for easy interpretation and analysis.\n",
        "\n",
        "This capability is essential for scalable inference and integrating MLflow models into data pipelines or applications that process multiple records at once.\n",
        "\n",
        "### Key Considerations\n",
        "\n",
        "* **`MLFLOW_RUN_ID` and `MODEL_ARTIFACT_PATH`:** As before, these must accurately point to the desired logged model.\n",
        "* **Batch Input Format:** When providing multiple data points, they should be structured as rows in a Pandas DataFrame. Each row represents a single instance for which a prediction is desired, and all columns must correspond to the features the model was trained on.\n",
        "* **Output:** The `predict()` method will return an an array of predictions, where each element corresponds to the prediction for the respective row in the input DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c9a643",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12abf3e84b1148ac964069a2c30ec7c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61dee272bcee474ebafb8291dbca0999",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating loaded model prediction from Run ID: e29e2b05b8e341d7809c89725c6797e9 ---\n",
            "Loaded pipeline from MLflow URI: runs:/e29e2b05b8e341d7809c89725c6797e9/model\n",
            "New passenger data:\n",
            "   Pclass     Sex   Age  SibSp  Parch  Fare Embarked\n",
            "0       1  female  25.0      1      0  75.0        C\n",
            "1       3    male  35.0      0      0   8.0        S\n",
            "2       2  female  40.0      2      1  45.0        Q\n",
            "Predicted Survival for all passengers (0=No, 1=Yes):\n",
            "[1 0 1]\n",
            "\n",
            "DataFrame with Predictions:\n",
            "   Pclass     Sex   Age  SibSp  Parch  Fare Embarked  Predicted_Survival\n",
            "0       1  female  25.0      1      0  75.0        C                   1\n",
            "1       3    male  35.0      0      0   8.0        S                   0\n",
            "2       2  female  40.0      2      1  45.0        Q                   1\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fastapi-deployment-explanation",
      "metadata": {},
      "source": [
        "# MLflow: FastAPI Deployment\n",
        "\n",
        "## Deploying a Logged MLflow Model as a Web API with FastAPI\n",
        "\n",
        "This section demonstrates how to deploy a machine learning model, previously logged with MLflow, as a RESTful API using FastAPI. FastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary goal is to create a simple, runnable web service that can receive new data and return predictions using the MLflow-logged model. This involves:\n",
        "\n",
        "1.  **FastAPI Application Setup:** Initializing a FastAPI application.\n",
        "2.  **Model Loading on Startup:** Implementing an `@app.on_event(\"startup\")` function to load the MLflow model only once when the API starts. This prevents redundant model loading for every request, improving performance.\n",
        "3.  **Input Data Validation with Pydantic:** Defining a `Pydantic` `BaseModel` (`TitanicPassenger`) to strictly validate the structure and types of incoming request data. This ensures that the input data conforms to the features expected by our trained model.\n",
        "4.  **Prediction Endpoint:** Creating a POST endpoint (`/predict`) that accepts passenger data, converts it into a Pandas DataFrame (the format expected by our `scikit-learn` pipeline), uses the loaded MLflow model to make a prediction, and returns the result.\n",
        "5.  **Robust Error Handling:** Including `try-except` blocks to catch potential issues during model loading or prediction, providing informative HTTP error responses.\n",
        "\n",
        "This setup is ideal for integrating your MLflow models into larger applications, microservices architectures, or providing real-time inference capabilities.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "* **`mlflow`:** Used for loading the pre-trained model from its MLflow URI.\n",
        "* **`fastapi`:** The web framework for building the API.\n",
        "* **`pydantic`:** Used by FastAPI for data validation and serialization/deserialization.\n",
        "* **`uvicorn`:** An ASGI server that runs the FastAPI application.\n",
        "* **`pandas`:** For structuring input data into a DataFrame format expected by the `scikit-learn` pipeline.\n",
        "\n",
        "**Before Running:**\n",
        "\n",
        "Make sure you have `mlflow`, `fastapi`, `uvicorn`, and `pandas` installed:\n",
        "```bash\n",
        "pip install mlflow fastapi uvicorn pandas\n",
        "```\n",
        "\n",
        "**Important Note on `MLFLOW_RUN_ID`:**\n",
        "\n",
        "You **must** replace the placeholder `MLFLOW_RUN_ID` with the actual Run ID of one of your trained models from the previous MLflow experiments (e.g., from the Titanic CV & Model Logging experiment). This ID tells FastAPI which specific model to load from your MLflow tracking server or local `mlruns` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa8e81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# app.py - This file defines the FastAPI application for model deployment\n",
        "\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Replace with the actual Run ID of your BEST model from MLflow UI!\n",
        "# This ID links to the specific run where your 'final_model_pipeline' was logged.\n",
        "# Example: MLFLOW_RUN_ID = \"a1b2c3d4e5f67890abcd1234\"\n",
        "MLFLOW_RUN_ID = \"e29e2b05b8e341d7809c89725c6797e9\" # <--- REPLACE THIS LINE WITH YOUR RUN ID!\n",
        "\n",
        "# Path to the model artifact within the run's artifacts directory.\n",
        "# This should match the name given when the model was logged (e.g., \"final_model_pipeline\").\n",
        "MODEL_ARTIFACT_PATH = \"model\"\n",
        "\n",
        "# Construct the MLflow model URI. This URI tells MLflow where to find the model.\n",
        "# If using a remote MLflow Tracking Server, you might need to set its URI first:\n",
        "# mlflow.set_tracking_uri(\"http://your_mlflow_server_ip:5000\")\n",
        "MODEL_URI = f\"runs:/{MLFLOW_RUN_ID}/{MODEL_ARTIFACT_PATH}\"\n",
        "\n",
        "# Initialize FastAPI app instance.\n",
        "app = FastAPI(\n",
        "    title=\"Titanic Survival Prediction API\",\n",
        "    description=\"API for predicting survival on the Titanic using an MLflow-logged model.\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Global variable to hold the loaded model pipeline.\n",
        "# It's initialized to None and loaded once on application startup.\n",
        "model_pipeline = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    \"\"\"\n",
        "    Load the MLflow model pipeline when the FastAPI application starts up.\n",
        "    This is an asynchronous function that runs once at the beginning.\n",
        "    It ensures the model is loaded only once and is available for all incoming requests,\n",
        "    avoiding redundant loading and improving API performance.\n",
        "    If the model fails to load, it raises an HTTPException, preventing the server from starting.\n",
        "    \"\"\"\n",
        "    global model_pipeline # Declare that we are modifying the global 'model_pipeline' variable\n",
        "    try:\n",
        "        print(f\"Loading model from MLflow URI: {MODEL_URI}\")\n",
        "        # Use mlflow.sklearn.load_model because the model was logged using mlflow.sklearn flavor.\n",
        "        model_pipeline = mlflow.sklearn.load_model(MODEL_URI)\n",
        "        print(\"Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Raise an HTTP exception to indicate a critical startup failure.\n",
        "        raise HTTPException(status_code=500, detail=f\"Model failed to load: {e}. \"\n",
        "                                                      f\"Please check MLFLOW_RUN_ID and ensure MLflow tracking server \"\n",
        "                                                      f\"is running correctly for local runs or configured for remote.\")\n",
        "\n",
        "# Define the input data schema using Pydantic.\n",
        "# This ensures that incoming JSON requests conform to the expected structure and data types.\n",
        "class TitanicPassenger(BaseModel):\n",
        "    Pclass: int\n",
        "    Sex: str\n",
        "    Age: float\n",
        "    SibSp: int\n",
        "    Parch: int\n",
        "    Fare: float\n",
        "    Embarked: str\n",
        "\n",
        "    # Example schema for OpenAPI documentation (Swagger UI).\n",
        "    class Config:\n",
        "        schema_extra = {\n",
        "            \"example\": {\n",
        "                \"Pclass\": 3,\n",
        "                \"Sex\": \"male\",\n",
        "                \"Age\": 28.0,\n",
        "                \"SibSp\": 0,\n",
        "                \"Parch\": 0,\n",
        "                \"Fare\": 10.0,\n",
        "                \"Embarked\": \"S\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Define the prediction endpoint.\n",
        "@app.post(\"/predict\")\n",
        "async def predict_survival(passenger: TitanicPassenger):\n",
        "    \"\"\"\n",
        "    Predicts survival for a single Titanic passenger based on the provided data.\n",
        "    Expects a JSON body conforming to the TitanicPassenger schema.\n",
        "    Returns the predicted survival status (0 or 1) and a human-readable string.\n",
        "    \"\"\"\n",
        "    # Check if the model has been successfully loaded. If not, return a 503 Service Unavailable error.\n",
        "    if model_pipeline is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded yet. Please wait for startup or check logs.\")\n",
        "\n",
        "    # Convert the incoming Pydantic model data into a Pandas DataFrame.\n",
        "    # This is crucial because scikit-learn pipelines typically expect DataFrame input.\n",
        "    # .model_dump() is used for Pydantic v2; for v1, it would be .dict()\n",
        "    input_df = pd.DataFrame([passenger.model_dump()]) \n",
        "\n",
        "    try:\n",
        "        # Make a prediction using the loaded MLflow pipeline.\n",
        "        # The pipeline handles all preprocessing steps internally.\n",
        "        prediction = model_pipeline.predict(input_df)[0]\n",
        "        survival_status = \"Survived\" if prediction == 1 else \"Not Survived\"\n",
        "        \n",
        "        # Return the prediction and a descriptive status.\n",
        "        return {\"prediction\": int(prediction), \"survival_status\": survival_status}\n",
        "    except Exception as e:\n",
        "        # Catch any errors during the prediction process and return a 500 Internal Server Error.\n",
        "        raise HTTPException(status_code=500, detail=f\"Prediction failed: {e}. \"\n",
        "                                                      f\"Ensure input data matches training features and types.\")\n",
        "\n",
        "# You can add a root endpoint for basic health check or info\n",
        "@app.get(\"/\", tags=[\"Health Check\"])\n",
        "async def root():\n",
        "    return {\"message\": \"Titanic Survival Prediction API is running. Visit /docs for API documentation.\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-fastapi-instructions",
      "metadata": {},
      "source": [
        "### Run the FastAPI Application\n",
        "\n",
        "To run this FastAPI application:\n",
        "\n",
        "1.  **Save the code:** Save the Python code from the previous cell into a file named `app.py` in your working directory.\n",
        "2.  **Open your terminal:** Navigate to the directory where you saved `app.py`.\n",
        "3.  **Execute the command:** Run the following command:\n",
        "    ```bash\n",
        "    uvicorn app:app --reload\n",
        "    ```\n",
        "    * `app:app` refers to the `app` object within the `app.py` file.\n",
        "    * `--reload` enables auto-reloading, which is useful during development as it restarts the server whenever code changes are detected.\n",
        "\n",
        "You should see output indicating that Uvicorn is running the server, typically on `http://127.0.0.1:8000` or `http://localhost:8000`.\n",
        "\n",
        "### Test Your API\n",
        "\n",
        "Once the server is running, you can test your API using a web browser or a tool like `curl` or Postman:\n",
        "\n",
        "-   **Root Endpoint:** Open your web browser and go to: `http://localhost:8000/`\n",
        "    * You should see a simple JSON response: `{\"message\": \"Titanic Survival Prediction API is running. Visit /docs for API documentation.\"}`\n",
        "\n",
        "-   **Interactive API Documentation (Swagger UI):** Open your web browser and go to: `http://localhost:8000/docs`\n",
        "    * FastAPI automatically generates interactive API documentation (Swagger UI). You can see your `/predict` endpoint, the expected input schema, and example requests. You can use the \"Try it out\" button to send sample requests directly from your browser and see the responses.\n",
        "\n",
        "-   **Alternative Documentation (ReDoc):** For a different documentation style, visit: `http://localhost:8000/redoc`\n",
        "\n",
        "Once the model has been selected and deployed with FastAPI, you can then access it via web clients or programmatically via HTTP requests."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calling-mlflow-with-request-explanation",
      "metadata": {},
      "source": [
        "### Calling the Deployed MLflow Model with Python `requests`\n",
        "\n",
        "This section demonstrates how to programmatically interact with the FastAPI model deployment using Python's `requests` library. This is how other applications or services would typically consume your deployed machine learning model.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The goal is to show how to send a JSON payload to the `/predict` endpoint of your running FastAPI application and process the JSON response. This includes:\n",
        "\n",
        "1.  **Defining the API Endpoint:** Specifying the URL of your FastAPI prediction endpoint.\n",
        "2.  **Preparing Request Headers:** Setting the `Content-Type` header to `application/json` to indicate that the request body is in JSON format.\n",
        "3.  **Creating the Request Body:** Constructing a Python dictionary that matches the `TitanicPassenger` Pydantic schema defined in your `app.py`.\n",
        "4.  **Sending the POST Request:** Using `requests.post()` to send the data to the API.\n",
        "5.  **Handling the Response:** Checking the HTTP status code and parsing the JSON response from the API.\n",
        "\n",
        "This client-side code simulates how a front-end application, another microservice, or a batch processing script would interact with your deployed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "342e1b48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction successful:\n",
            "{'prediction': 0, 'survival_status': 'Not Survived'}\n"
          ]
        }
      ],
      "source": [
        "import requests # Import the requests library for making HTTP requests\n",
        "\n",
        "# --- Configuration for API Call ---\n",
        "url = \"http://localhost:8000/predict\" # The URL of your running FastAPI prediction endpoint\n",
        "headers = {\"Content-Type\": \"application/json\"} # Specify that we are sending JSON data\n",
        "\n",
        "# The data payload for a single passenger, matching the TitanicPassenger Pydantic model.\n",
        "data = {\n",
        "    \"Pclass\": 3,\n",
        "    \"Sex\": \"male\",\n",
        "    \"Age\": 28.0,\n",
        "    \"SibSp\": 0,\n",
        "    \"Parch\": 0,\n",
        "    \"Fare\": 10.0,\n",
        "    \"Embarked\": \"S\"\n",
        "}\n",
        "\n",
        "# Send the POST request to the FastAPI endpoint.\n",
        "# The 'json' parameter automatically serializes the Python dict to JSON and sets the Content-Type header.\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "# Check the HTTP status code of the response.\n",
        "if response.status_code == 200:\n",
        "    print(\"Prediction successful:\")\n",
        "    # Parse the JSON response body and print it.\n",
        "    print(response.json())\n",
        "else:\n",
        "    # If the request was not successful, print the status code and the error message.\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n",
        "    print(\"Please ensure the FastAPI application (app.py) is running and accessible at http://localhost:8000.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4a9967",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Replicate the Churn project using MLFlow. Test different algorithms like Logistic Regression, Random forest and Gradient Boost Classifier, select the best one and deploy it with MLFlow directly and also with FastAPI. Take the Titanic example as a baseline to build your solution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce866e5",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
